
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}
%This is a template for producing LIPIcs articles. 
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Dummy title} %TODO Please add

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Jane {Open Access}}{Dummy University Computing Laboratory, [optional: Address], Country \and My second affiliation, Country \and \url{http://www.myhomepage.edu} }{johnqpublic@dummyuni.org}{https://orcid.org/0000-0002-1825-0097}{(Optional) author-specific funding acknowledgements}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional. Use additional curly braces to indicate the correct name splitting when the last name consists of multiple name parts.

\author{Joan R. Public\footnote{Optional footnote, e.g. to mark corresponding author}}{Department of Informatics, Dummy College, [optional: Address], Country}{joanrpublic@dummycollege.org}{[orcid]}{[funding]}

\authorrunning{J. Open Access and J.\,R. Public} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Jane Open Access and Joan R. Public} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Dummy keyword} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\nota}[3]{{%
		\color{#2}
		\marginpar{\color{#2!75!black}\textbf\texttimes}%
		\textsf{\textbf{[\textbullet#1:}
			\textsf{\small#3}
			\textbf{\textbullet]}}%
}}
\newcommand{\domagoj}[1]{\nota{domagoj}{red!55!black}{#1}}
\newcommand{\ignore}[1]{}
\newcommand{\todo}[1]{\nota{TODO}{red!55!black}{#1}}
\newtheorem{idea}{Idea}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent convallis orci arcu, eu mollis dolor. Aliquam eleifend suscipit lacinia. Maecenas quam mi, porta ut lacinia sed, convallis ac dui. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse potenti. 
\end{abstract}

\section{Typesetting instructions -- Summary}
\label{sec:typesetting-summary}

LIPIcs is a series of open access high-quality conference proceedings across all fields in informatics established in cooperation with Schloss Dagstuhl. 
In order to do justice to the high scientific quality of the conferences that publish their proceedings in the LIPIcs series, which is ensured by the thorough review process of the respective events, we believe that LIPIcs proceedings must have an attractive and consistent layout matching the standard of the series.
Moreover, the quality of the metadata, the typesetting and the layout must also meet the requirements of other external parties such as indexing service, DOI registry, funding agencies, among others. The guidelines contained in this document serve as the baseline for the authors, editors, and the publisher to create documents that meet as many different requirements as possible. 

Please comply with the following instructions when preparing your article for a LIPIcs proceedings volume. 
\paragraph*{Minimum requirements}

\begin{itemize}
\item Use pdflatex and an up-to-date \LaTeX{} system.
\item Use further \LaTeX{} packages and custom made macros carefully and only if required.
\item Use the provided sectioning macros: \verb+\section+, \verb+\subsection+, \verb+\subsubsection+, \linebreak \verb+\paragraph+, \verb+\paragraph*+, and \verb+\subparagraph*+.
\item Provide suitable graphics of at least 300dpi (preferably in PDF format).
\item Use BibTeX and keep the standard style (\verb+plainurl+) for the bibliography.
\item Please try to keep the warnings log as small as possible. Avoid overfull \verb+\hboxes+ and any kind of warnings/errors with the referenced BibTeX entries.
\item Use a spellchecker to correct typos.
\end{itemize}

\paragraph*{Mandatory metadata macros}
Please set the values of the metadata macros carefully since the information parsed from these macros will be passed to publication servers, catalogues and search engines.
Avoid placing macros inside the metadata macros. The following metadata macros/environments are mandatory:
\begin{itemize}
\item \verb+\title+ and, in case of long titles, \verb+\titlerunning+.
\item \verb+\author+, one for each author, even if two or more authors have the same affiliation.
\item \verb+\authorrunning+ and \verb+\Copyright+ (concatenated author names)\\
The \verb+\author+ macros and the \verb+\Copyright+ macro should contain full author names (especially with regard to the first name), while \verb+\authorrunning+ should contain abbreviated first names.
\item \verb+\ccsdesc+ (ACM classification, see \url{https://www.acm.org/publications/class-2012}).
\item \verb+\keywords+ (a comma-separated list of keywords).
\item \verb+\relatedversion+ (if there is a related version, typically the ``full version''); please make sure to provide a persistent URL, e.\,g., at arXiv.
\item \verb+\begin{abstract}...\end{abstract}+ .
\end{itemize}

\paragraph*{Please do not \ldots} %Do not override the \texttt{\seriesstyle}-defaults}
Generally speaking, please do not override the \texttt{lipics-v2021}-style defaults. To be more specific, a short checklist also used by Dagstuhl Publishing during the final typesetting is given below.
In case of \textbf{non-compliance} with these rules Dagstuhl Publishing will remove the corresponding parts of \LaTeX{} code and \textbf{replace it with the \texttt{lipics-v2021} defaults}. In serious cases, we may reject the LaTeX-source and expect the corresponding author to revise the relevant parts.
\begin{itemize}
\item Do not use a different main font. (For example, the \texttt{times} package is forbidden.)
\item Do not alter the spacing of the \texttt{lipics-v2021.cls} style file.
\item Do not use \verb+enumitem+ and \verb+paralist+. (The \texttt{enumerate} package is preloaded, so you can use
 \verb+\begin{enumerate}[(a)]+ or the like.)
\item Do not use ``self-made'' sectioning commands (e.\,g., \verb+\noindent{\bf My+ \verb+Paragraph}+).
\item Do not hide large text blocks using comments or \verb+\iffalse+ $\ldots$ \verb+\fi+ constructions. 
\item Do not use conditional structures to include/exclude content. Instead, please provide only the content that should be published -- in one file -- and nothing else.
\item Do not wrap figures and tables with text. In particular, the package \texttt{wrapfig} is not supported.
\item Do not change the bibliography style. In particular, do not use author-year citations. (The
\texttt{natbib} package is not supported.)
\end{itemize}

\enlargethispage{\baselineskip}

This is only a summary containing the most relevant details. Please read the complete document ``LIPIcs: Instructions for Authors and the \texttt{lipics-v2021} Class'' for all details and don't hesitate to contact Dagstuhl Publishing (\url{mailto:publishing@dagstuhl.de}) in case of questions or comments:
\href{http://drops.dagstuhl.de/styles/lipics-v2021/lipics-v2021-authors/lipics-v2021-authors-guidelines.pdf}{\texttt{http://drops.dagstuhl.de/styles/lipics-v2021/\newline lipics-v2021-authors/lipics-v2021-authors-guidelines.pdf}}

\section{Overview}

Bichromatic closest pair was first introduced in~\cite{Pankaj1990} where Pankaj et al. give a reduction of euclidean minimum spannig tree to two colored bichromatic closest pair and compare their hardness.
They also provide a randomized algorithm for computing the bichromatic closest pair which runs in expected $O((nm)^{1-\frac{1}{\lceil \frac{d}{2} \rceil + 1} + \varepsilon} + m \log n + n \log m)$ for any $\varepsilon > 0$ and $n$ the size of the first color's set and $m$ the size of the second color's set.
Later Aggrawal et al.~\cite{Aggrawal1992} give an optimal algorithm to solve the $k$-colored bichromatic all nearest neighbors problem in plane.
Their aproach uses a Voronoi diagaram to compute the neighbors and has a running time of $O(n \log n)$ for any number of colors.
This in turn gives a $O(n \log n)$ solution to the bichromatic closest pair.
Khuller and Matias~\cite{Khuller1995} give a randomized algorithm for the closest pair problem that runs in expected $O(n)$ time which, they argue, can be extended to the bichromatic closest pair problem in higher dimensions with a running time still in expected $O(n)$.
In the same year Eppstein~\cite{Eppstein1995} gives important algorithms which solve the dynamic Euclidean minimum spanning tree and dynamic bichromatic closest pair.
The running time of maintaining the Euclidean minimum spannig tree and two colored bichromatic closest pair is $O(n^{1 - \varepsilon})$ where $\varepsilon > 0$ depends on the dimension $d$.
For rectilinear metrics, the MST can be maintained in time $O(\sqrt{n} \log^d n)$.
Dumitrescu and Guha~\cite{Dumitrescu2002} gave an MST approach for the $k$-colored bichromatic closest pair in plane with running time $O(n \log n)$ and reduced the $k$-colored bichromatic closest pair problem to $\lceil \log k \rceil$ two-colored bichromatric closest pair problems.
They also give an algorithm that maintains a bichromatic closest pair under color change with running time $O(\log n)$ per update in any dimension.
In the same manner, Borgelt and Borglet~\cite{Borgelt2008} give an algorithm that maintains in plain a $k$-colored bichromatic all nearest neighbors with running time $O(n\log n)$ to construct a Voronoi diagram and amortized $O(\log n)$ time to update it.
With Impagliazzo nad Paturi~\cite{Impagliazzo2001} stating the Strong Exponential Time Hypothesis (SETH) we have conditional lower-bounds on some problems.
Williams~\cite{Williams2005} proved that SETH implies Orthogonal Vectors Hypothesis (OVH).
O'Donnell et al.~\cite{ODonnell2009} gave strict bounds on the locality sensitive hashing approach.
Alman et al.~\cite{Alman2016} give a randomized $(1+\varepsilon)$-approximate all nearest neighbor with expected running time $O(dn + n^{2 - 1/\Omega(\sqrt[3]{\varepsilon}/\log \frac{1}{\varepsilon})})$ for $l_1$ and Euclidean metrics.
With Bringmann's introduction of Fine-grained complexity theory~\cite{Bringmann2019,Bringmann2021} bichromatic closest pair was researched from a purely theoretical standpoint.
Improving on to Alman et al.~\cite{Alman2016}, Rubinstein~\cite{Rubinstein2018} proved a better conditional lower-bound on the $(1+\varepsilon)$-approximate bichromatic closest pair with time $O(n^{2-\varepsilon})$, which in turn implies better conditional lower-bounds for approximate bichromatic closest pair queries and approximate bichromatic all nearest neighbors.
He also pointed out similar results for Hamming distnace and Fr\'{e}chte distance, whereas for $l_1$ and $l_\infty$ exist subquadratic algorithms.
Pagh et al.~\cite{Pagh2019} proved similar results for $(1+\varepsilon)$-approximate bichromatic closest pair with Jaccard similarity.
In a different approach, Flores-Velazco and Mount~\cite{FloresVelazco2021} studied the classification and give a datastructure that can query the most occuring color of the $k$ $\varepsilon$-nearest neighbors of a given point.
Horst et al.~\cite{Horst2022} also provide a datastructure that can answer the most occuring color of the exact $k$ nearest neigbors of a given point.

\section{Reduction proof}
\begin{theorem}
The $k$-colored bichromatic closest pair problem can be reduced to $\lceil \log k \rceil$ $2$-colored bichromatic closest pair problems.
\end{theorem}
\begin{proof}
  Let $k$ be the number of different colors and let $A_1$, $A_2$, $\dots$, $A_k$ be sets of points coresponding to different colors.
  To make the proof cleaner, let us relable the sets to $A_0$, $A_1$, $\dots$, $A_{k-1}$.
  Now construct $\lceil \log k \rceil$ pairs $p_i$ with
  \[ p_i := \left(\bigcup \limits_{j = 0..(k-1) \wedge i\text{-th bit of }j \text{ is }0} A_j, \bigcup \limits_{j = 0..(k-1) \wedge i\text{-th bit of } j \text{ is }1} A_j\right). \]
  For $i > \lceil \log k \rceil$, pairs will be of form $( \bigcup A_j, \emptyset)$ and so they are not needed.
  By computing the $2$-colored bichromatic closest pair between the left and right set of each pair $p_i$ and taking the minimal solution, we get a solution to the $k$-colored bichromatic closest pair.
  To prove this, assume the $k$-colored bichromatic closest pair is achieved between sets $A_{r_1}$ and $A_{r_2}$.
  Since $r_1 \neq r_2$ there has to exist a bit in which $r_1$ and $r_2$ differ.
  Let that be the $s$-th bit.
  Since they differ in the $s$-th bit, they are split appart in the pair $p_s$.
  Now the $2$-colored bichromatic pair on this pair $p_s$ will return this solution (or one equally good).
  And the minimum over all of the pairs should be as good as this solution.
  
\end{proof}

\section{Bichromatic closest pair}

\begin{definition}[bichromatic closest pair]
For a given dimension $d$, and two sets of points $A, B \subset \mathbb{R}^d$ where $A$ are red colored points and $B$ are blue colored points, compute the
$$ \min \limits_{p \in A,\ q\in B} d_2(p, q) $$
where $d_2(p, q)$ is the Euclidean distance.
\end{definition}

\begin{definition}[bichromatic closest pair with k-colors]
For a given dimension $d$, number of colors $k$, and sets $S_1, S_2, \dots S_k \subset \mathbb{R}^d$,
compute
$$ \min \limits_{p \in S_i,\ q \in S_j,\ i \neq j} d_2(q, p). $$

\end{definition}

This problem was introduced in \todo{Geometry book} as a two colored version.
Later it was studied from a theoretical aspect because of its hardness \todo{(see SETH and OVH)}.
All bichromatic nearest neighbor has its use in practice in a fine number of problems such as \todo{LIST SOME PROBLEMS}.
Generally there is no well known algorithm that solves it and is speculated that there can not exist an algorithm that solves it efficiently for general dimension.
Special cases such as dynamic bichromatic nearest neighbor queries on plane, reverse bichromatic all nearest neighbors, or the most occuring color in some points k-neighborhood, have algorithms that solve them.
Special cases on plane, such as the above mentioned dynamic bichromatic nearest neighbor queries, are solvable in optimal time. \todo{See reduction to Does there exist a copy of two real numbers in multiset}
In this paper we provide a simple-to-implement algorithm that solves the general d-dimensional k-colored all bichromatic nearest neighbor with acceptable running time that is also adaptable to solving its variants.
The whole setup would look like a framework, so to say, and the ideas behind it are the following.
\begin{idea} We would like to reduce the general k-colored variant of this problem to a all-nearest neighbor problem or to a 2-colored bichromatic nearest neighbor problem, since these problems are well known.
\end{idea}
\begin{idea}\label{idea:bigcup} For each point in some set $A_i$ we would like to decompose the whole dataset into some (possibly small) number of disjoint sets $\bigcup\limits_j B_j = A \backslash A_i$ so we could use algorithms for the aforementioned problems.
\end{idea}
The idea~\ref{idea:bigcup} leads to a well known algorithm and data structure used in information theory.

\begin{theorem}[Huffman coding]\label{tm:huff}
Given a distribution $p_1, p_2, \dots p_k$, we can find in time $O(k \log k)$ a complete binary tree such that
$$ \sum \limits_{i = 1}^k p_i \log_2 \frac{1}{p_i} \leq \sum \limits_{i = 1}^k p_i l_i \leq 1 + \sum \limits_{i = 1}^k p_i \log_2 \frac{1}{p_i} $$
where $l_i$ is depth of a leaf assigned to the $i$-th distribution.
\end{theorem}
Theorem \ref{tm:huff} is a well known result in information theory and its proof is located in \todo{Cormen et al}.
Lower bound of \ref{tm:huff} is due to Shannon, and its upper bound and greedy construction algorithm are due to Huffman.
Using this data structure we will try to "encode" each color such that the decomposition from idea \ref{idea:bigcup} has at most $O\left(\log n\right)$ disjoint subsets, and we will try to do if for each color.
We can do that thanks to the above theorem in the following way.
\begin{theorem}[Encoded colors tree]\label{tm:logn}
For a given dimension $d$, number of colors $k$ and sets $A_1, A_2, \dots A_k \subset \mathbb{R}^d$ we can construct a complete binary tree with leaves associated to sets
$A_i,\ \forall i = 1 \dots n$ and internal nodes associated to union of sets in its subtree in time $O\left(d\, n \log n\right)$, where $n = \sum \limits_{i = 1}^k |A_i|$ is the number of all points.
\end{theorem}
\begin{proof}
Let us denote the size of each set with $w_i = |A_i|$ and note that $w_i$ is the number of times set $A_i$ will be copied.
For these $w_i$ we define $p_i := \frac{w_i}{n}$.
$p_i$ make up a distribution which means we can use theorem \ref{tm:huff} to construct a binary tree with leaves associated to distributions $p_i$ i.e. sets $A_i$.
During construction we copy each point from the current node to its parent so that root of each subtree has a union of points of its leaves.
Each point from set $A_i$ we need to copy $l_i$ times and this will take $O(d\, l_i)$ operations.
The set $A_i$ we can copy in time $O(d\, w_il_i)$.
For all sets our running time is $ \sum \limits_{i=1}^k O(d\,w_i l_i)$
 By theorem \ref{tm:huff} these $l_i$ are such that
$$ \sum \limits_{i = 1}^k p_i l_i \leq \sum \limits_{i = 1}^k p_i \log_2 \frac{1}{p_i}. $$
By expanding $p_i$ we have
\begin{align*}
	\sum \limits_{i = 1}^t \frac{w_i}{n} l_i
	&\leq 1 + \sum \limits_{i = 1}^t \frac{w_i}{n} \log_2 \frac{n}{w_i} \\
	&\leq 1 + \sum \limits_{i = 1}^t \frac{w_i}{n}\log_2 \frac{n}{\min \limits_{i} w_i} \\
	&= 1 + \frac{1}{n}\log_2 \frac{n}{\min \limits_i w_i} \sum \limits_{i = 1}^t w_i \\
	&\leq 1 + \frac{1}{n}\log_2 \frac{n}{\min \limits_i w_i} n \\
	&= 1 + \log_2 \frac{n}{\min \limits_i w_i} \\
	&\leq 1 + \log_2 n, \\ 
	\sum \limits_{i=1}^t w_i l_i &\leq n \left(1 + \log_2 n\right).
\end{align*}
From this we can see that our running time is $O\left(d\,n \log n\right)$.\\
\end{proof}

\begin{remark} For a balanced dataset with $|A_i| = n/k$ we have a running time of $O\left(d\,n \log k\right)$.
If we want to remove the $d$ from the running time we can copy just the indices.
If we want to optimize it even more we can sort the whole dataset of points by the size of each colored set, and use $O(n)$ Huffman coding algorithm in which we will save only indexes of the first and last point of that subset.
\end{remark}
To use this data structure in the way we planed to we need a little bit of additional information.
The whole process of additional processing and finding is quite easy and is described in the following theorem.
\begin{theorem}\label{tm:decompose}
With the data structure described in theorem \ref{tm:logn} we can for each point from a set $A_i$ find a decomposition of disjoint sets $\bigcup\limits_j B_j = A \backslash A_i$ in amortized $O(\log n)$ time.
\end{theorem}
\begin{proof}
The idea is to precompute hashes using a {\it dfs} in which we always descend into the left child first and in each leaf we save a number, at which time we visited it.
For each color we have its visit time $time_i$ saved in its leaf.
Then for every node we save a number $maxTime = \max \left(leftChild.maxTime, rightChild.maxTime\right)$ that denotes the max $time_i$ in its subtree.
For leaves $maxTime = time_i$.
This number we can compute while returning from the {\it dfs} recursion and overall cost will be $O(t)$ which is in worst-case $O(n)$.
Now to find a target leaf for point $i$ we do the following:
\begin{algorithm}
\caption{Find($T$, $i$)}\label{alg:decompose}
\begin{algorithmic}
    \If {$isLeaf(T)$}
    \State return $\emptyset$.
    \ElsIf {$T.leftChild.maxTime < time[i]$}
    \State return $\left\{T.leftChild.set \right\} \bigcup Find\left(T.rightChild,i\right)$.
    \Else
    \State return $Find\left(T.leftChild, i\right) \bigcup \left\{T.rightchild.set\right\}$.
    \EndIf
\end{algorithmic}
\end{algorithm}

By theorem \ref{tm:huff} we will descend exactly $l_i$ times for each point.
For all points we will descend overall
$\sum \limits_{i = 1}^k w_il_i = O\left(n \log n\right)$ times which gives $O(\log n)$ amortized time per point.
We can easily verify by induction that this algorithm \ref{alg:decompose} returns a set of sets who in union give $\bigcup\limits_j B_i = A \backslash A_i$ for a target point $i$, and as such is left as an exercise for the reader.
\end{proof}

We have a data structure, now we need to make it usefull.
Given theorems \ref{tm:logn} and \ref{tm:decompose} we can go in two directions: online algorithm and offline algorithm.
Since this data structure can be view as a binary search tree it seems natural to first explain the online solution for this problem.
Now the first thing we need to have for this solution is an algorithm or data structure that has $n \cdot P\left(n\right)$ precompute time and can answer {\bf Which point in this set of points is nearest to some given point not living in this set?} in time $Q(n)$ where $n$ is the size of mentioned set.
Using this we get the following two theorems.


\begin{theorem}[Construction time]\label{tm:construct}
Given the Encoded Colors Tree from theorem \ref{tm:logn} and a data structure with construction time $O\left(nP_d\left(n\right)\right)$, $P_d(n)$ monotonically increasing, we need overall $O\left(nP_d(n)\log n\right)$ time to build this data structure inside every node of the Encoded Colors Tree.
\end{theorem}
\begin{proof}
Since Huffman trees are complete binary trees we have $2k - 1$ nodes which gives a naive upper bound of $O\left(n^2P_d(n)\right)$ construction time.
Having Huffman tree properties we can do better.
Let again $w_i := |A_i|$ and let $b_j$ be the size of the set inside node $j$, $\forall j = 1\dots 2k-1$.
Our construction time of building the given data structure inside all of the nodes is asymptotically bounded above by
$$ \sum\limits_{j=1}^{2k - 1} b_jP_d(b_j) $$
which by monotonicity of $P_d(n)$ is
$$ \sum\limits_{j=1}^{2k - 1} b_jP_d(b_j) \leq
P_d(n) \sum\limits_{j=1}^{2k - 1} b_j. $$
By definition of $b_j$ and construction of Encoded Colors tree we have
$$ \sum\limits_{j = 1}^{2k - 1} b_j = \sum\limits_{i = 1}^n w_il_i $$
which in turn gives us an asymptotic upper bound of
$$ \sum\limits_{j = 1}^{2k - 1} b_jP_d(b_j) = O\left( P_d(n)n \log n\right). $$
\end{proof}

\begin{remark}
A slight optimization can be made if the datastructure can be split on merged when building the tree.
\end{remark}

\begin{theorem}[Query time]\label{tm:query}
Given the augmented Encoded Colors Tree from theorem \ref{tm:construct} whose data structure can answer queries of the form {\bf Find the point inside this set that is closest to the given query point} in time $O\left(Q_d(n)\right)$, $Q_d(n)$ monotonically increasing, we can answer in amortized $O\left(Q_d(n) \log n\right)$ time {\bf Find the nearest bichromatic neighbor of the given query point}.
\end{theorem}
\begin{proof}
Proof is trivial and follows from \ref{tm:decompose} and \ref{tm:construct}.
Let again $w_i := |A_i|$ and $b_j$ be the size of set in node $j$.
Using algorithm \ref{alg:decompose} from \ref{tm:decompose} we find $l_i$ disjoint sets for our query point in time $\Theta\left(l_i\right)$.
Each of these sets can find the closest point to our query point in time $Q_d\left({b_i}_j\right)$ which means we have an asymptotic upper bound of
$$ \sum\limits_{j = 1}^{l_i} Q_d\left({b_i}_j\right) \leq \sum\limits_{j = 1}^{l_i}Q_d\left(n\right) = Q\left(n\right)l_i. $$
Now since we found the closest point in each of disjoint sets $B_j$, who in union give $A\backslash A_i$, we know that the closest point of those $l_i$ points is the nearest bichromatic neighbor to our query point, thus we have our answer.
Now the overall running time to query all points is bounded above by
$$ \sum\limits_{i = 1}^n w_i \sum\limits_{j = 1}^{l_i} Q_d\left({b_i}_j\right)
    \leq \sum\limits_{i = 1}^n w_iQ_d\left(n\right)l_i = Q_d\left(n\right) \sum\limits_{i = 1}^n w_il_i = O\left(Q_d(n)n\log n\right) $$
which in turn gives us an amortized running time of $O\left(Q_d(n)\log n\right)$ per query.
\end{proof}

\begin{remark}
An optimization can be made by passing information from the above node to the below nodes when doing a query.
Information may help in some cases for example when passing the current best distance we can limit the search space for other calculations.
Information passing can be done in the reverse manner where we go from leaf to root and propagate results upwards.
\end{remark}

We summarize the above two theorems into the following corollary.
\begin{corollary}\label{cor:online}
For some dimension $d$, given a data structure with precompute time $O\left(n\, P_d(n)\right)$
that can answer find the nearest neighbor in time $O\left(Q_d(n)\right)$, we can solve the $k$-colored all bichromatic nearest neighbor in time $O\left(n \log n \left(P_d(n) + Q_d(n)\right)\right)$, independent of the number of colors.
\end{corollary}
\begin{proof} Theorems \ref{tm:construct} and \ref{tm:query} \end{proof}
\begin{remark}
Given superadditive $P_d(n)$ and $Q_d(n)$ we can easily show the running of the above method to be $O(nQ_d(n) + nP_d(n))$ which can be easily proven to be $\Omega(n^2)$.
\end{remark}
\begin{remark}
In terms of Master Theorem we can view the running time of the whole procedure as a recursion of the form
$$ T(n) = 2T(n/2) + O(P(n) + Q(n)), $$
which is the case for datasets with same sized colored sets.
\end{remark}

Corollary \ref{cor:online} has a constructive proof and, in terms of fine-grained complexity theory, gives a reduction to Nearest Neighbors Search problem.
In mathematical terms that would mean the following.
\begin{corollary}
If nearest neighbor search can be solved in time $O(n^{1-\varepsilon_1}poly(d))$ then $k$-colored all bichromatic nearest neighbor can be solved in time $O(n^{2-\varepsilon_2}poly(d))$, independent of the number of colors.
\end{corollary}
This information isn't anything new.
Nearest neighbor search was extensively research in the past decade and a lot of results were proven for conditional hardness of it and similar problems.
\todo{CITE A LOT OF WORK from Rubinstein.}
Our framework given above is also extendable to $k$-nearest bichromatic neghbors, bichromatic closest pair, approximate bichromatic nearest neighbor, randomized bichromatic nearest neighbor, in general $l_p$ metric bichromatic nearest neighbor, and so on, given the appropriate data structure.
This fact implies a lot of hardness results e.g. approximate nearest neighbor search in $O(n^{1-\varepsilon})$ implies approximate bichromatic nearest neighbor search in $O(n^{1-\varepsilon})$\todo{cite Rubinstine}.

One that we explained the online approach of this framework, we can easily adapt it to an offline approach in which we in bulk calculate the nearest neighbors for sets of points in the following manner.

\begin{theorem}[$k\to 2$]\label{tm:k2color}
Given the Encoded Colors Tree form theorem \ref{tm:logn} and an algorithm $Alg(S_1, S_2)$ that can solve the 2-color all bichromatic nearest neighbor, we can solve the $k$-color all bichromatic nearest neighbor.
\end{theorem}
\begin{proof}
The algorithm is straightforward and is similar to the one in \ref{tm:query}.
\begin{algorithm}
\caption{Calculate($T$)}\label{alg:k2color}
\begin{algorithmic}
    \If {$isLeaf(T)$}
    \State return $\emptyset$.
    \Else
    \State $rec\_results := Calculate(T.leftChild) \bigcup Calculate(T.righChild)$,
    \State return $\min\left(rec\_results, Alg\left(T.lefChild.set, T.righChild.set\right)\right)$.
    \EndIf
\end{algorithmic}
\end{algorithm}
Each left child is disjoint to the right child and in union give the whole subset located in the respective subtree.
\end{proof}
\begin{remark}
Using $Alg$ with running time $f(b_1,b_2)$, we can have a running time of $O(f(n,n)\log n)$ or $O(f(n,n))$ for the $k$-color all bichromatic nearest neighbor, depending on the properties of $f(b_1,b_2)$.
\end{remark}
The offline version of this framework is in principal the same as the online version.
The same generalization can be made for this case i.e. we can extend this offline version to $k$-color bichromatic closest pair problem, approximate bichromatic closest pair problem, randomized

\begin{corollary}
$k$-color all bichromatic nearest neighbor is as hard as $2$-color all bichromatic nearest neighbor and the running time doesn't depend on the number of colors.
\end{corollary}

\section*{Approximate bichromatic nearest neighbor }

Let  $A_1$, $A_2$, $\dots$, $A_t \subset \mathbb{R}^d$. In {\it all bichromatic approximate nearest neighbor} problem, for each $i = 1 \dots t$ and each $q \in A_i$, we want to find 
a point $ z \in \bigcup \limits_{i \neq j} A_j$ such that $d_2(q,p)\le d_2(q,z) \le (1+\varepsilon) d_2(q, p)$, where 
$
	p \in \underset{{p \in \bigcup \limits_{i \neq j} A_j}}{\arg \min }\;  d_2 \left(q, p\right).
$
We will show that all bichromatic approximate nearest neighbor problem can be computed in $O(n\log n)$ time with the 
help of the well-separated pair decomposition, a well-know geometric decomposition introduced in a seminar work of 
Callahan and Kosaraju in 1995 \cite{callahan95-phd, callahan95}.

\paragraph{Well-separated pair decomposition.} Let $S$ be a set of $n$ points in $\mathbb{R}^d$. For any $A \subseteq S$, let $R(A)$ denote the minimum enclosing axis-aligned box of $A$.  Let $C_A$ be the minimum enclosing ball of $R(A)$, and let 
$r(A)$ denote the radius of $C_A$.  Let $C^{r(A)}$ be the ball with the same center as 
$C_A$, but with radius $r(A)$.  Furthermore, for two sets $A$, $B \subseteq S$, let $r = \max(r(A), r(B))$, and let $d(A, B)$ denote the minimum distance between $C^{r}_A$ and $C^{r}_B$.  For example, if the $C_A$ intersects $C_B$, then $d(A,B) = 0$.

\begin{definition}
A pair of sets $A$ and $B$ are said to be well-separated if $d(A, B) > s \cdot r$, for 
any given separation constant $s>0$ and $r = \max\{r(A), r(B)\}$. 
\end{definition}

\ignore{
\begin{figure}
  \centering
  \includegraphics[width=.63\linewidth]{figures/dumbell.png} 
  \caption{An example of a dumbbell for a pair of sets $(A, B)$. }
  \label{fig:fig1}
\end{figure}
}%ignore
 

\begin{definition}[WSPD]\label{wspd:def}
A well-separated pair decomposition of $S\subset \mathbb{R}^d$, for a given $s>0$, is 
a sequence $(A_1, B_1), \ldots, (A_k, B_k)$, where $A_i, B_i\subseteq S$, such that 
\begin{enumerate}
    \item $A_i, B_i$ are well-separated with respect to separation constant $s$, for all $i=1,\ldots, k$;
    \item for all $p\neq q\in S$ there exists a unique pair $(A_i, B_i)$ such that 
    $p\in A_i, q\in B_i$ or $q\in A_i, p\in B_i$. 
\end{enumerate}
\end{definition}
Note that WSPD always exists since one could use all singleton pairs $(\{p\},\{q\})$, 
for all pairs $p, q\in S$. However, this would yield a sequence of dumbbells of size
$k = \Theta(n^2)$. The  question is whether one could do better than that. The 
answer to that question was given by the following theorem.
\begin{theorem}[\cite{callahan95}]\label{tm:WSPD}
Given a set $S$ of $n$ points in $\mathbb{R}^d$ and a separation constant $s>0$, 
a WSPD of $S$ with $O(s^d d^{d/2}n)$ many dumbbells can be computed in 
$O(dn\log n + s^d d^{d/2}n)$.
\end{theorem}

We will choose the separation constant $s$ and modify the construction of WSPD such that it can be reused in the context 
of our problem. 

Let $(A_i, B_i)$, $i=1, \ldots, k$, denote the WSPD for some set of points $S\subset \mathbb{R}^d$. 
For $a, a' \in A_i$ and $b, b' \in B_i$ for some dumbbell $i$, we make the following observations:
\begin{enumerate}
    \item Points within the sets $A_i$ and $B_i$ can be made 'arbitrarily close' as compared to points in the opposite sets by choosing the appropriate 
    separation $s>0$, i.e. 
        \begin{equation}
            d(a, a') \le 2 r < \frac{2}{s} d(A_i, B_i) \le \frac{2}{s} d(a, b) .
        \end{equation}
    \item Distances between points in the opposite sets can be made 'almost equal', by choosing the appropriate $s>0$, i.e. 
        \begin{equation}\label{eq:wspd_guarantee}
                d(a', b') \le d(a, a') + d(a, b) + d(b, b') < (1+\frac{4}{s}) d(a, b).
        \end{equation}
\end{enumerate}
Thus, for $s = \frac{4}{\varepsilon}$, we have that $d(a', b')\le (1+\varepsilon) d(a, b)$, for any $\varepsilon>0$.



\paragraph{Construction of WSPD with an additional color information.} For the construction of WSPD the split tree of a 
set $S$ is computed by Algorithm~\ref{alg:splittree}.
\begin{algorithm}
\caption{SplitTree($S$)}\label{alg:splittree}
\begin{algorithmic}
    \If {$\textrm{size}(S) = 1$} 
    \State return $\textrm{leaf}(S)$.
    \Else
    \State Partition $S$ into sets $S_1$ and $S_2$ by halving $R(S)$ with hyperplane along its longest side. 
    \State Return a node with children (SplitTree($S_1$), SplitTree($S_2$).
    \EndIf
\end{algorithmic}
\end{algorithm}
Even though such a tree might have linear depth and therefore a naive construction of the split tree by Algorithm~\ref{alg:splittree} in the worst case takes
quadratic time. However, the work of \cite{callahan95} showed how to compute such a binary tree in $O(n\log n)$ time. With every 
node $u$ of that tree we can conceptually associate the set $S_u$ of all points contained in its subtree. Node $u$
is called colorful node if $S_u$ contains points from two or more colors. Otherwise, we say that $S_u$ is monochromatic. 

\begin{definition}[Colorful edge]
Let $u$ and $w$ denote two nodes in a split tree such that the corresponding 
set of points $(S_u, S_w)$ for a well-separated pair. If either $S_u$ or $S_w$ is colorful, then the ($S_u$, $S_w$) is called
colorful edge. If both $S_u$ and $S_w$ are monochromatic, but colored with different colors, then ($S_u$, $S_w$) is called colorful edge. 
\end{definition}
In order to compute the subset of WSPD consisting only of colorful edges, for each internal node $u$ of the split tree $T$ with
children $v,w$ and $v_l, v_r$ and $w_l, w_r$ denoting left and right child of $v,w$ respectively, we invoke Algorithm~\ref{alg:coloredpairs}. 
\begin{algorithm}
\caption{FindColorfulEdges($v, w$)}\label{alg:coloredpairs}
\begin{algorithmic}
\Require Split tree $T$ of $S$, separation constant $s>0$
\If {$S_v$ and $S_w$ are well-separated and $(S_v, S_w)$ colorful}
    \State add colorful edge $(v, w)$ to the tree $T$.
\ElsIf{$L_{\max}(S_v) > L_{\max}(S_w)$}
    \State FindColorfulEdges($v_l$, $w$), FindColorfulEdges($v_r$, $w$)
\Else
    \State FindColorfulEdges($v$, $w_l$), FindColorfulEdges($v$, $w_r$)
\EndIf
\end{algorithmic}
\end{algorithm}
Let $C_S$ denote the set of colorful edges computed by Algorithm~\ref{alg:coloredpairs}. 
Note that $C_S$ is a subset of WSPD of $S$. Furthermore, for 
any node $u$ of the split tree $T$, let $C_{S_u}\subseteq C_S$ denote the
set of all colorful edges such that $(S_w, S_v)\in C_S$, for nodes $v, w \in T$, and 
$v$ ancestor of $u$. We 
consider $u$ to be an ancestor of itself. 
Let $(S_{w'}, S_u) = \arg\min_{(S_w, S_v)\in C_{S_u}} d(S_w, S_v)$, i.e. $(S_{w'}, S_u)$ is 
the 'shortest' colorful edge in $C_{S_u}$. We save that information with every node $v\in T$. 
\begin{theorem}
Given a set $S$ of $n$ points in $R^d$, AND $k$ COLORS,  the approximate 
all-bichromatic nearest neighbors problem can be solved in $O(n \log n)$ time. 
\end{theorem}
\begin{proof}
Let $p$ by any point in $S$ and let $q\in S$ be its bichromatic nearest neighbor. Let $u$
denote the leaf in the split tree that stores $q$, and let $(S_{w'}, S_u$ is the 
shortest colorful edge saved with $u$. Then $p\in $
\end{proof}

%%
%% Bibliography
%%

%% Please use bibtex, 

\bibliography{references}

\appendix

\section{Styles of lists, enumerations, and descriptions}\label{sec:itemStyles}

List of different predefined enumeration styles:

\begin{itemize}
\item \verb|\begin{itemize}...\end{itemize}|
\item \dots
\item \dots
%\item \dots
\end{itemize}

\begin{enumerate}
\item \verb|\begin{enumerate}...\end{enumerate}|
\item \dots
\item \dots
%\item \dots
\end{enumerate}

\begin{alphaenumerate}
\item \verb|\begin{alphaenumerate}...\end{alphaenumerate}|
\item \dots
\item \dots
%\item \dots
\end{alphaenumerate}

\begin{romanenumerate}
\item \verb|\begin{romanenumerate}...\end{romanenumerate}|
\item \dots
\item \dots
%\item \dots
\end{romanenumerate}

\begin{bracketenumerate}
\item \verb|\begin{bracketenumerate}...\end{bracketenumerate}|
\item \dots
\item \dots
%\item \dots
\end{bracketenumerate}

\begin{description}
\item[Description 1] \verb|\begin{description} \item[Description 1]  ...\end{description}|
\item[Description 2] Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\item[Description 3]  \dots
%\item \dots
\end{description}

\cref{testenv-proposition} and \autoref{testenv-proposition} ...

\section{Theorem-like environments}\label{sec:theorem-environments}

List of different predefined enumeration styles:

\begin{theorem}\label{testenv-theorem}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{theorem}

\begin{lemma}\label{testenv-lemma}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{lemma}

\begin{corollary}\label{testenv-corollary}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{corollary}

\begin{proposition}\label{testenv-proposition}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{proposition}

\begin{conjecture}\label{testenv-conjecture}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{conjecture}

\begin{observation}\label{testenv-observation}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{observation}

\begin{exercise}\label{testenv-exercise}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{exercise}

\begin{definition}\label{testenv-definition}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{definition}

\begin{example}\label{testenv-example}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{example}

\begin{note}\label{testenv-note}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{note}

\begin{note*}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{note*}

\begin{remark}\label{testenv-remark}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{remark}

\begin{remark*}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{remark*}

\begin{claim}\label{testenv-claim}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{claim}

\begin{claim*}\label{testenv-claim2}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{claim*}

\begin{proof}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{proof}

\begin{claimproof}
Fusce eu leo nisi. Cras eget orci neque, eleifend dapibus felis. Duis et leo dui. Nam vulputate, velit et laoreet porttitor, quam arcu facilisis dui, sed malesuada risus massa sit amet neque.
\end{claimproof}

\end{document}
